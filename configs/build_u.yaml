model_name: meta-llama/Llama-3.1-8B-Instruct
dataset_name: wikitext
dataset_config: wikitext-103-raw-v1
fallback_dataset_config: wikitext-2-raw-v1
split: train
text_field: text
num_sequences: 5000
batch_size: 1
max_seq_len: 256
tokens_per_batch: 256
r_pca: 16
seed: 42
layer_range: middle_third
out_path: artifacts/u_subspaces.pt
log_every: 100
