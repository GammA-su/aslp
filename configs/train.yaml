model_name: meta-llama/Llama-3.1-8B-Instruct
train_jsonl: data/domain_train.jsonl
eval_jsonl: data/domain_eval.jsonl
mode: baseline
seed: 42
max_seq_len: 256
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2.0e-4
max_steps: 100
logging_steps: 10
eval_steps: 50
save_steps: 50
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
r_pca: 16
u_path: artifacts/u_subspaces.pt
output_dir: ""
proj_log_steps: 20
