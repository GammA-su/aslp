Metadata-Version: 2.4
Name: aslp
Version: 0.1.0
Summary: ASLP regression-resistant finetuning experiment
Requires-Python: >=3.12
Description-Content-Type: text/markdown
Requires-Dist: torch==2.4.1
Requires-Dist: transformers==4.46.2
Requires-Dist: peft==0.13.2
Requires-Dist: datasets==2.20.0
Requires-Dist: accelerate>=1.1.0
Requires-Dist: evaluate==0.4.3
Requires-Dist: tqdm==4.66.5
Requires-Dist: pyyaml==6.0.2
Requires-Dist: numpy==1.26.4
Requires-Dist: pytest==8.3.2

# ASLP LoRA Regression-Resistant Finetuning

Activation-Subspace Left-Projection (ASLP) protects general capabilities during LoRA finetuning by removing update components aligned with a protected activation subspace. A PCA basis U is built from frozen model activations on general text, then after each optimizer step we project the LoRA left factor B to remove components in span(U). A random-subspace control uses the same projection with random orthonormal bases to match compute.

## Quickstart

export HF_TOKEN=...   # required for meta-llama/Llama-3.1-8B-Instruct
uv venv
uv lock
uv sync
uv run python scripts/make_toy_domain_data.py --out_dir data
uv run python scripts/build_u.py --config configs/build_u.yaml
uv run python scripts/train_lora.py --config configs/train.yaml --mode baseline
uv run python scripts/train_lora.py --config configs/train.yaml --mode aslp
uv run python scripts/train_lora.py --config configs/train.yaml --mode random
uv run python scripts/eval_ppl.py --config configs/eval.yaml --eval_type general --mode baseline --lora_path artifacts/runs/baseline_seed42 --out_json artifacts/runs/baseline_seed42/general.json
uv run python scripts/eval_ppl.py --config configs/eval.yaml --eval_type domain --mode baseline --eval_jsonl data/domain_eval.jsonl --lora_path artifacts/runs/baseline_seed42 --out_json artifacts/runs/baseline_seed42/domain.json
uv run python scripts/summarize_results.py --run_dir artifacts/runs/baseline_seed42 --run_dir artifacts/runs/aslp_seed42 --run_dir artifacts/runs/random_seed42
make build_u MODEL=meta-llama/Llama-3.1-8B-Instruct
make train MODE=baseline SEED=42 MODEL=meta-llama/Llama-3.1-8B-Instruct
make eval_general MODE=baseline SEED=42 MODEL=meta-llama/Llama-3.1-8B-Instruct

## Artifacts

- `artifacts/u_subspaces.pt` contains per-module U bases and metadata.
- `artifacts/runs/{mode}_seed{seed}` contains LoRA checkpoints and eval JSON outputs.
- `data/` contains the toy domain JSONL files.

## Notes

- Set `HF_TOKEN` for gated model access. The scripts pass it to Hugging Face APIs.
- Defaults are small so the demo runs on a single GPU; edit YAMLs for larger runs.
